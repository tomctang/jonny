{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cW61j1Dw9ABL"
   },
   "source": [
    "# Introduction\n",
    "\n",
    "I'm a Sydney boy from NSBHS. I'm currently working on [Permutation Puzzles](../10pe/Course_Puzzle.ipynb) and [Project Euler](../10pe/PE_Informatics/PE100.ipynb). \n",
    "\n",
    "For completed courses, see my [LinkedIn certificates](https://www.linkedin.com/in/jonny-jingzhe-tang-853a57274/details/certifications/). Any questions, please email: jonny.tang745@education.nsw.gov.au"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vj6j6iPy9ABV"
   },
   "source": [
    "\n",
    "> **Mathematics & FP(Functional Programming)/Language** (updating)\n",
    "\n",
    "I have completed 3U & 4U Math, and A-level & further Math as my transition btw secondary and tertiary education:\n",
    "- [A-level math](https://www.edx.org/learn/math/imperial-college-london-a-level-mathematics-for-year-12-course-1-algebraic-methods-graphs-and-applied-mathematics-methods), including four courses\n",
    "- [A-level further math](https://www.edx.org/learn/math/imperial-college-london-a-level-further-mathematics-for-year-12-course-2-3-x-3-matrices-mathematical-induction-calculus-methods-and-applications-maclaurin-series-complex-numbers-and-polar-coordinates), including four courses\n",
    "\n",
    "I further studied undergraduate Math in both computation and proof dimensions:\n",
    "- [Math Computational Methods](https://www.edx.org/learn/math/georgetown-university-mathematical-and-computational-methods): learning Math through Physics\n",
    "- [Lean](../10pe/Course_Lean.ipynb): FP language, five Lean games for math proofs, First-order logic(FOL), revolutionary tool for both math and all science subjects\n",
    "\n",
    "Now I am working on a perfect course that integrates combinatorics, group theory, and SageMath:\n",
    "- [Permutation Puzzles - Group Theory](https://www.youtube.com/watch?v=WLtiyLGAtKo&list=PLKXCdnugmHRm2ICudfhC9xco1p350mma2)\n",
    "\n",
    "> **CS & Physics** (updating)\n",
    "\n",
    "I already finished two MIT CS-courses in 2024, and two physics-courses in 2025. My notes are included in this workbook:\n",
    "- [Quantum Mechanics](https://www.edx.org/learn/quantum-physics-mechanics/georgetown-university-quantum-mechanics), including a baby course for everyone, and uni-level QM course\n",
    "- [MITx-6.00.1x-Introduction to Computer Science and Programming Using Python](https://learning.edx.org/course/course-v1:MITx+6.00.1x+1T2024/home)\n",
    "- [MITx-6.00.2x-Introduction to Computational Thinking and Data Science](https://learning.edx.org/course/course-v1:MITx+6.00.2x+1T2024/home)\n",
    "\n",
    "After two MIT courses, I started **Project Euler** (PE) and this workbook with Tom (my Dad) in 2024. PE problems require two streams of knowledge:\n",
    "\n",
    "- Sometimes, I face math problems while coding is easy, e.g. 99-Largest Exponential. See [PE-Tags&Math](../10pe/PE_Informatics/PE_tags.ipynb) where I illustrated math tags from PE system;\n",
    "- Sometimes, I face computational (or programming) problems while math is easy, e.g. 187-Semiprimes (very time-consuming). See [CS or Programming](https://github.com/leduckhai/Awesome-Competitive-Programming?tab=readme-ov-file)\n",
    "\n",
    "I love coding, number theory, and OEIS, especially the PE-style (VS. CS-style). Tom supervised and assisted me with his knowledge guidance and searching.\n",
    "\n",
    "> **Notebooks**: Geography, Maps, Lottery math\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Notes by Tom**\n",
    "\n",
    "Tom supervising Jonny's learning by [**integrating math language, coding language and natural languages**](https://tomctang.github.io/oeis/mathmap_old00intro_math.html).\n",
    "\n",
    "Bad ideas| Good ideas\n",
    "|-|-|\n",
    "distinguish math btw high-school(secondary edu) and uni(tertiary edu) | Combo of 2nd and 3rd-level math topics\n",
    "distinguish math and science(especially physics)| Combo of math and physics\n",
    "distinguish math and CS(computer science)| Combo of 'conventional' and computational math\n",
    "distinguish math and natural language| Combo of natural language, math language and coding language\n",
    "\n",
    "So: \n",
    "- Thinking in Math\n",
    "- Writing in coding \n",
    "- Sharing in natural language"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Timeline & QA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Timeline**\n",
    "\n",
    "2023-24, I went through 3U & 4U Math (NSW-HSC) in paper-pen approach. I also used A-level & further Math for reviewing 3U4U and adopting the computational/coding approach. \n",
    "\n",
    "2025: go through uni-level multi-variable calculus, linear algebra, differential equations in a course preparing for quantum mechanics \n",
    "\n",
    "> **For help**\n",
    "\n",
    "- For questions about phys, try [Physics Stack Exchange](https://physics.stackexchange.com/). For research-level questions, [Physics Overflow.](https://physicsoverflow.org/)\n",
    "- For questions about math, try [Math Stack Exchange](https://math.stackexchange.com/). for research-level questions, [Math Overflow.](https://mathoverflow.net/)\n",
    "- For more free-wheeling discussions of math and physics, try [Physics Forums.](https://www.physicsforums.com/)\n",
    "\n",
    "> **Image-Hosting**\n",
    "\n",
    "All figures are synchronised in Tom's GitHub folder `./NB_img/`. Using codes to insert pic:\n",
    "\n",
    "`<img src=\"https://raw.githubusercontent.com/tomctang/NB_img/main/3bias.png\" alt=\"Causality-confounder\" width=\"500\">`\n",
    "\n",
    "Markdown image **center**:\n",
    "\n",
    "`<div align=center>![Figure. Various treatments choice for OA](https://raw.githubusercontent.com/ctang83/NB_img/main/EPS_OAtreat.jpg){width=60%}`\n",
    "\n",
    "> **Cross-citations**\n",
    "\n",
    "Header automatically generate {#mathFP}, so`[anytext](#mathFP)` in citing\n",
    "\n",
    "Between pages: `[Discrete & Continuous](../10pe/02discrete_continuous.ipynb#discrete-continuous)`(ipynb auto changed to html in website)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ubuntu & SCE (Scientific Computing Environment) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Ubuntu is the key in SCE. To build a SCE is the first step to learn programming, two ways:\n",
    "1. Cloud Computing - **Google Colab, Cocalc(Rust, Julia, GNU Octave(vs.Matlab)), [Binder](https://mybinder.org/v2/gh/tomctang/10pe/HEAD)** - are good, but Colab and Binder require further installation (originally Python)\n",
    "1. Self-built - **Docker**'s images and containers (self-maintained not self-installation) are the best\n",
    "    - I already build SageMath and IHaskell using Docker, both running `ipynb`\n",
    "1. Key to Science - reproducibility, so we need a managing Environment and Container\n",
    "    1. Use one environment(or Container) per project! Creating environments to accommodate specific workflows/projects — and to do so early on\n",
    "    1. Containers is better, which package tools with underlying operating system, are larger and more complicated than environments, but are more portable\n",
    "\n",
    "Steps:\n",
    "1. install Ubuntu\n",
    "1. install Chrome\n",
    "    - sudo apt-get install libxss1 libappindicator1 libindicator7\n",
    "    - wget https://dl.google.com/linux/direct/google-chrome-stable_current_amd64.deb\n",
    "    - sudo apt install ./google-chrome*.deb\n",
    "1. git:`sudo apt install git`\n",
    "    - download github project\n",
    "        - git clone weblink\n",
    "    - for github conflicted problem\n",
    "        - git stash\n",
    "        - git pull --rebase\n",
    "        - git stash pop\n",
    "1. Install Docker\n",
    "    - sudo apt  install curl \n",
    "    - curl -fsSL https://get.docker.com -o get-docker.sh\n",
    "    - sh get-docker.sh\n",
    "    - dockerd-rootless-setuptool.sh install\n",
    "    - `docker ps ; docker images ; `\n",
    "1. <del>Google Drive - rclone</del>, now Dropbox & Syncthing (super-fast@home) <del>\n",
    "    - sudo -v ; curl https://rclone.org/install.sh | sudo bash\n",
    "    - run \"rclone config\", follow guide: https://rclone.org/drive/\n",
    "    - run \"sudo nano /etc/fuse.conf\" Uncomment this line: user_allow_other\n",
    "    - rclone mount gdrive: /home/tom/gdrive --allow-other --vfs-cache-mode full\n",
    "    - automatic boot: 2 issues - connection & mount\n",
    "        - Test rclone connection: `rclone lsd remote:`\n",
    "        - sudo nano /etc/systemd/system/rclone-gdrive.service # see T430's file\n",
    "        - systemctl daemon-reload\n",
    "        - sudo systemctl enable rclone-gdrive.service\n",
    "        - sudo systemctl start rclone-gdrive.service</del>\n",
    "1. <del>uni-direction synchronize two folders `rsync -avu --delete source_folder/ destination_folder/`</del>\n",
    "2. [WineHQ for Foxit PDF](https://gitlab.winehq.org/wine/wine/-/wikis/Debian-Ubuntu)\n",
    "3. My PC has two disks, I installed Ubuntu goes along original windows in the first disk (original sys). Now the second disk can't seen as Ubuntu still hardly manage Microsoft Windows dynamic disk (LDM). Solutions:\n",
    "    - ldmtool to read data in Ubuntu, or wait Ubuntu updates in the future\n",
    "    - convert LDM to basic disk while lossing all data\n",
    "4.  Basic commands & more:\n",
    "    - `ls -a ;  env  ;  pwd  ; rm ; echo > aa.hs (create file); nano aa.hs (edit file, Ctrl+X for quit)`\n",
    "    - run Ubuntu commands in GHCI, add `:!` ahead\n",
    "    - remove top bar: `sudo apt install gnome-shell-extension-manager`, then search \"hide top bar\"\n",
    "    - PDF arranger (App Center): edit PDF\n",
    "    - printer [brother driver](https://support.brother.com/g/b/downloadend.aspx?c=au&lang=en&prod=hll2350dw_us_eu_as&os=128&dlid=dlf103566_000&flang=4&type3=10283)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More\n",
    "\n",
    "> **PL**\n",
    "\n",
    "While a qualified coder mastering 6 programming languages (PL), how to choose PLs is really a hard decision.\n",
    "\n",
    "1. We started this workbook using **Python** in VScode:\n",
    "    - Python: we began from Python, then we moved to Math Tools, like MMA and SageMath(the second best CAS - computer algebra system).\n",
    "    - [diagrams](https://diagrams.mingrammer.com/): Coding diagrams\n",
    "    - [manim](https://www.manim.community/): Math Animation\n",
    "    - [handcalcs](https://github.com/connorferster/handcalcs): render calculation as if it were written with a pencil: write the symbolic formula, then numeric substitutions, and then show result automatically\n",
    "1. Next, a PL with more FP? Ocaml? F#(Fsharp)? Scala? Haskell? Rust? Make a comparison:\n",
    "    1. PL's Coding-look in defining a Fibonacci; \n",
    "    2. Which FP is best for mathematics? Haskell is NO.1, but I really don't like Haskell; \n",
    "    3. Math-related supports for these four languages; \n",
    "    4. Balance between FP and OOP; \n",
    "    5. Number of users in Project Euler for each PL; \n",
    "    6. libgen supports for each PL; \n",
    "    7. When the PL first appeared.\n",
    "    - Decision: **Rust & Haskell**. \n",
    "        - Google endorse [Codeworld - Educational Haskell programming platform](https://code.world/) with [Github Repository](https://github.com/google/codeworld/tree/master)\n",
    "\n",
    "> **Math Tools**\n",
    "\n",
    "1. Three large-scale mathematical software packages\n",
    "    1. Numerical computing systems: floating point, accurate estimation, numerical analysis. Matlab, Octave\n",
    "    2. CAS(Computer algebra system): exact computation, view $\\pi$ as a symbol instead of a decimal number, symbolic computation/computer algebra/computational algebra\n",
    "    3. Statistical packages: SAS, R\n",
    "1. Mathematics and Self-Study Roadmaps\n",
    "    - [The Map of Mathematics](https://www.youtube.com/watch?v=OmJ-4B-mS-Y)\n",
    "    - [How to self study pure math - a step-by-step guide](https://www.youtube.com/watch?v=byNaO_zn2fI)\n",
    "1. Instead of Python, then we started **Mathematica (Wolfram languages, MMA or Wolfram** after):\n",
    "    - We are running Wolfram in ipynb-file using VScode\n",
    "    - Mathematica (.nb): the best CAS. Shall we convert .nb -> .ipynb ??\n",
    "    - installing\n",
    "        - [Process Github](https://github.com/CraverBoyyy/CosCOM-Mathematica-Installation?tab=readme-ov-file)\n",
    "        - [Key](https://wu-yijun.github.io/Mathematica-Keygen-Mechanism/)\n",
    "1. SageMath isn't a pip-installable package, instead it's large and requires C/Fortran/Python... lots of pre-installed environments\n",
    "    - SageMath is the best open-source CAS\n",
    "    - [CoCalc (.ipynb)](https://cocalc.com/projects) online platform includes all open-source computing environments, so surprising!!\n",
    "    - [Google Colab](https://stackoverflow.com/questions/49685498/how-do-you-import-sagemath-in-google-colaboratory/77440661) can also install SageMath, but [with problems](https://github.com/demining/Install-SageMath-in-Google-Colab)\n",
    "    - or **Docker + SageMath + JupyterLab** to work with symbolic mathematics programmatically\n",
    "    - Sage installed by miniconda is the best option\n",
    "        - we can use self-built functions every time starting Sage, like s()\n",
    "        - go to folder `/home/tom/.sage`, open file `init.sage`\n",
    "    - Resources:\n",
    "        - [Sage Interactions](https://wiki.sagemath.org/interact)\n",
    "        - [All SageMath tutorials](https://doc.sagemath.org/)\n",
    "1. Julia is the best for DE, and is also [more FP than python - reason 1](https://github.com/jkrumbiegel/Chain.jl), [reason 2](https://github.com/thautwarm/MLStyle.jl)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(*Convert nb file into vsnb using the wl coding*)\n",
    "Get[\"_static/Mathematica2VSCode.wl\"] \n",
    "Needs[\"Mathematica2VSCode`\"]\n",
    "Mathematica2VSCode[\"a/1_MOESM1.nb\"]  (*path and specific MMA notebook*)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Organization & Management**\n",
    "\n",
    "1. Website Management\n",
    "    - Git and GitHub\n",
    "    - automatic deploy from a branch `gh-pages`\n",
    "1. Project Management\n",
    "    - Visual Studio Code\n",
    "    - Jupyter notebooks (.ipynb): interactive computational notebooks, easy to prototype, create visualizations, interactive presentations\n",
    "    - MarkDown and Latex: writing natural language and math formulus\n",
    "1. Citations and Bibliographies\n",
    "    - Zotero: a free, easy-to-use tool to help you collect, organize, cite, and share research. I use Zotero to generate .bib files\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OOP to FP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We started workbook using Python in OOP (Object-oriented Programming) way, then we move to FP (Functional Programming) way in both MMA and Python.\n",
    "- From my view, OOP coding is more natural language, FP is more math lanuage.\n",
    "- Even though both Python and Wolfram are not pure FP, they do help us understand the FP way.\n",
    "\n",
    "FP resources\n",
    "1. [FP books](https://old.reddit.com/r/functionalprogramming/wiki/books)\n",
    "1. [Adabeat - Ease into](https://adabeat.com/fp/ease-into-functional-programming/?_gl=1*1r4s1nz*_up*MQ..*_ga*NDgxNzc4NDQ0LjE3MzA4MTAwMTg.*_ga_6RQG15HSGT*MTczMDgxMDAxNy4xLjAuMTczMDgxMDAxNy4wLjAuMA..)\n",
    "1. [PE solutions in Java, Python, Mathematica, Haskell](https://github.com/nayuki/Project-Euler-solutions): Nayuki use OOP in Python and FP in Haskell.\n",
    "\n",
    "FP - languages\n",
    "1. [FP in Python](https://expression.readthedocs.io/en/latest/intro.html): a web-notebook for beginner.\n",
    "1. [SageMath - FP4Mathematicians](https://doc.sagemath.org/html/en/thematic_tutorials/functional_programming.html#)\n",
    "1. [FP in Wolfram](https://reference.wolfram.com/language/guide/FunctionalProgramming.html): official website.\n",
    "\n",
    "e.g. Why not Matlab to model and simulate dynamic systems? – still OOP .VS. FP\n",
    "- Matlab(Simulink) uses a signal flow-based or causal approach(OOP), \n",
    "- while Modelica uses an equation-based or acausal approach(FP). \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pWioGU5SApZm"
   },
   "source": [
    "### Haskell\n",
    "\n",
    "`cabal init --interactive`(Haskell project)\n",
    "    \n",
    ">IHaskell in Ubuntu\n",
    "\n",
    "1.  A. step-by-step to avoid skipping; B.if using IHaskell packages, the ipynb must locate in IHaskell folder\n",
    "    - sudo apt-get install -y python3-pip git libtinfo-dev libzmq3-dev libcairo2-dev libpango1.0-dev libmagic-dev libblas-dev liblapack-dev\n",
    "    - curl -sSL https://get.haskellstack.org/ | sh  # install stack \n",
    "    - export PATH=\"/usr/local/bin:$PATH\"\n",
    "    - export PATH=\"/home/tom/.local/bin:$PATH\"\n",
    "    - git clone https://github.com/gibiansky/IHaskell \n",
    "    - cd IHaskell\n",
    "    - pip3 install -r requirements.txt -break-system-packages  # install Python requirements\n",
    "    - stack install --fast  # install ihaskell\n",
    "    - ihaskell install --stack   # install the Jupyter kernelspec with ihaskell\n",
    "    - Restart PC and make sure Jupyter access fully\n",
    "        - jupyter notebook --generate-config\n",
    "        - nano ~/.jupyter/jupyter_notebook_config.py\n",
    "        - add into: \n",
    "        - c.NotebookApp.token = ''\n",
    "        - c.NotebookApp.password = ''\n",
    "        - c.NotebookApp.allow_origin = '*'\n",
    "        - c.NotebookApp.allow_root = True\n",
    "    - make sure lts = 22.10 (consist to Github version)\n",
    "        - nano /home/tom/.stack/global-project/stack.yaml\n",
    "    - In IHaskell folder, run `stack exec jupyter -- notebook`\n",
    "        - or Click Jupyter icon at Dash\n",
    "\n",
    ">IHaskell in docker:\n",
    "\n",
    "1. IHaskell computation = **Docker + Haskell + Jupyter** (Docker building: 1. download [zip](https://github.com/IHaskell/IHaskell), 2. go to Docker terminal, run `\"docker build -t ihaskell:latest .\"`)\n",
    "    - Haskell's interpreter: IHaskell, [HyperHaskell](https://github.com/HeinrichApfelmus/hyper-haskell), ptGHCi\n",
    "    - I am using [ihaskell-notebook Docker image](https://github.com/IHaskell/ihaskell-notebook): JupyterLab version is low, unable to use AutoCompletion.\n",
    "    - can also use [IHaskell docker](https://github.com/IHaskell/IHaskell) and [Docker build py](https://gist.github.com/brandonchinn178/928d6137bfd17961b9584a8f96c18827)\n",
    "    - [IHaskell disscusion](https://github.com/IHaskell/IHaskell/issues/715) for reference\n",
    "\n",
    "<del>\n",
    "\n",
    ">wsl2-Ubuntu-Haskell in windows:\n",
    "\n",
    "1. Microsoft Store -> search 'Ubuntu' and install (345 MB)\n",
    "1. Open Ubuntu in Windows, update by:\n",
    "    - sudo apt update\n",
    "    - sudo apt install build-essential -y\n",
    "    - sudo apt install make -y\n",
    "    - sudo apt install automake -y\n",
    "\n",
    ">windows-msys2-Haskell in powershell:\n",
    "\n",
    "1. Two lauch commands:\n",
    "    - `ghci` is the interactive environment of the GHC compiler itself\n",
    "    - `stack ghci` is the interactive environment of the Stack build tool: 1) automatically handles dependency and installation (VS. GHC and Cabal), 2) config to install packages C:\\Users\\tangc\\AppData\\Roaming\\stack\\global-project\\stack.yaml\n",
    "</del>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GPT, LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete algorithm to train and inference a GPT. Everything else is just efficiency.\n",
    "import os       # os.path.exists\n",
    "import math     # math.log, math.exp\n",
    "import random   # random.seed, random.choices, random.gauss, random.shuffle\n",
    "random.seed(42) # Let there be order among chaos\n",
    "\n",
    "# Let there be an input dataset `docs`: list[str] of documents (e.g. a dataset of names)\n",
    "if not os.path.exists('input.txt'):\n",
    "    import urllib.request\n",
    "    names_url = 'https://raw.githubusercontent.com/karpathy/makemore/refs/heads/master/names.txt'\n",
    "    urllib.request.urlretrieve(names_url, 'input.txt')\n",
    "docs = [l.strip() for l in open('input.txt').read().strip().split('\\n') if l.strip()] # list[str] of documents\n",
    "random.shuffle(docs)\n",
    "print(f\"num docs: {len(docs)}\")\n",
    "\n",
    "# Let there be a Tokenizer to translate strings to discrete symbols and back\n",
    "uchars = sorted(set(''.join(docs))) # unique characters in the dataset become token ids 0..n-1\n",
    "BOS = len(uchars) # token id for the special Beginning of Sequence (BOS) token\n",
    "vocab_size = len(uchars) + 1 # total number of unique tokens, +1 is for BOS\n",
    "print(f\"vocab size: {vocab_size}\")\n",
    "\n",
    "# Let there be Autograd, to recursively apply the chain rule through a computation graph\n",
    "class Value:\n",
    "    __slots__ = ('data', 'grad', '_children', '_local_grads') # Python optimization for memory usage\n",
    "\n",
    "    def __init__(self, data, children=(), local_grads=()):\n",
    "        self.data = data                # scalar value of this node calculated during forward pass\n",
    "        self.grad = 0                   # derivative of the loss w.r.t. this node, calculated in backward pass\n",
    "        self._children = children       # children of this node in the computation graph\n",
    "        self._local_grads = local_grads # local derivative of this node w.r.t. its children\n",
    "\n",
    "    def __add__(self, other):\n",
    "        other = other if isinstance(other, Value) else Value(other)\n",
    "        return Value(self.data + other.data, (self, other), (1, 1))\n",
    "\n",
    "    def __mul__(self, other):\n",
    "        other = other if isinstance(other, Value) else Value(other)\n",
    "        return Value(self.data * other.data, (self, other), (other.data, self.data))\n",
    "\n",
    "    def __pow__(self, other): return Value(self.data**other, (self,), (other * self.data**(other-1),))\n",
    "    def log(self): return Value(math.log(self.data), (self,), (1/self.data,))\n",
    "    def exp(self): return Value(math.exp(self.data), (self,), (math.exp(self.data),))\n",
    "    def relu(self): return Value(max(0, self.data), (self,), (float(self.data > 0),))\n",
    "    def __neg__(self): return self * -1\n",
    "    def __radd__(self, other): return self + other\n",
    "    def __sub__(self, other): return self + (-other)\n",
    "    def __rsub__(self, other): return other + (-self)\n",
    "    def __rmul__(self, other): return self * other\n",
    "    def __truediv__(self, other): return self * other**-1\n",
    "    def __rtruediv__(self, other): return other * self**-1\n",
    "\n",
    "    def backward(self):\n",
    "        topo = []\n",
    "        visited = set()\n",
    "        def build_topo(v):\n",
    "            if v not in visited:\n",
    "                visited.add(v)\n",
    "                for child in v._children:\n",
    "                    build_topo(child)\n",
    "                topo.append(v)\n",
    "        build_topo(self)\n",
    "        self.grad = 1\n",
    "        for v in reversed(topo):\n",
    "            for child, local_grad in zip(v._children, v._local_grads):\n",
    "                child.grad += local_grad * v.grad\n",
    "\n",
    "# Initialize the parameters, to store the knowledge of the model.\n",
    "n_embd = 16     # embedding dimension\n",
    "n_head = 4      # number of attention heads\n",
    "n_layer = 1     # number of layers\n",
    "block_size = 16 # maximum sequence length\n",
    "head_dim = n_embd // n_head # dimension of each head\n",
    "matrix = lambda nout, nin, std=0.08: [[Value(random.gauss(0, std)) for _ in range(nin)] for _ in range(nout)]\n",
    "state_dict = {'wte': matrix(vocab_size, n_embd), 'wpe': matrix(block_size, n_embd), 'lm_head': matrix(vocab_size, n_embd)}\n",
    "for i in range(n_layer):\n",
    "    state_dict[f'layer{i}.attn_wq'] = matrix(n_embd, n_embd)\n",
    "    state_dict[f'layer{i}.attn_wk'] = matrix(n_embd, n_embd)\n",
    "    state_dict[f'layer{i}.attn_wv'] = matrix(n_embd, n_embd)\n",
    "    state_dict[f'layer{i}.attn_wo'] = matrix(n_embd, n_embd)\n",
    "    state_dict[f'layer{i}.mlp_fc1'] = matrix(4 * n_embd, n_embd)\n",
    "    state_dict[f'layer{i}.mlp_fc2'] = matrix(n_embd, 4 * n_embd)\n",
    "params = [p for mat in state_dict.values() for row in mat for p in row] # flatten params into a single list[Value]\n",
    "print(f\"num params: {len(params)}\")\n",
    "\n",
    "# Define the model architecture: a stateless function mapping token sequence and parameters to logits over what comes next.\n",
    "# Follow GPT-2, blessed among the GPTs, with minor differences: layernorm -> rmsnorm, no biases, GeLU -> ReLU\n",
    "def linear(x, w):\n",
    "    return [sum(wi * xi for wi, xi in zip(wo, x)) for wo in w]\n",
    "\n",
    "def softmax(logits):\n",
    "    max_val = max(val.data for val in logits)\n",
    "    exps = [(val - max_val).exp() for val in logits]\n",
    "    total = sum(exps)\n",
    "    return [e / total for e in exps]\n",
    "\n",
    "def rmsnorm(x):\n",
    "    ms = sum(xi * xi for xi in x) / len(x)\n",
    "    scale = (ms + 1e-5) ** -0.5\n",
    "    return [xi * scale for xi in x]\n",
    "\n",
    "def gpt(token_id, pos_id, keys, values):\n",
    "    tok_emb = state_dict['wte'][token_id] # token embedding\n",
    "    pos_emb = state_dict['wpe'][pos_id] # position embedding\n",
    "    x = [t + p for t, p in zip(tok_emb, pos_emb)] # joint token and position embedding\n",
    "    x = rmsnorm(x)\n",
    "\n",
    "    for li in range(n_layer):\n",
    "        # 1) Multi-head attention block\n",
    "        x_residual = x\n",
    "        x = rmsnorm(x)\n",
    "        q = linear(x, state_dict[f'layer{li}.attn_wq'])\n",
    "        k = linear(x, state_dict[f'layer{li}.attn_wk'])\n",
    "        v = linear(x, state_dict[f'layer{li}.attn_wv'])\n",
    "        keys[li].append(k)\n",
    "        values[li].append(v)\n",
    "        x_attn = []\n",
    "        for h in range(n_head):\n",
    "            hs = h * head_dim\n",
    "            q_h = q[hs:hs+head_dim]\n",
    "            k_h = [ki[hs:hs+head_dim] for ki in keys[li]]\n",
    "            v_h = [vi[hs:hs+head_dim] for vi in values[li]]\n",
    "            attn_logits = [sum(q_h[j] * k_h[t][j] for j in range(head_dim)) / head_dim**0.5 for t in range(len(k_h))]\n",
    "            attn_weights = softmax(attn_logits)\n",
    "            head_out = [sum(attn_weights[t] * v_h[t][j] for t in range(len(v_h))) for j in range(head_dim)]\n",
    "            x_attn.extend(head_out)\n",
    "        x = linear(x_attn, state_dict[f'layer{li}.attn_wo'])\n",
    "        x = [a + b for a, b in zip(x, x_residual)]\n",
    "        # 2) MLP block\n",
    "        x_residual = x\n",
    "        x = rmsnorm(x)\n",
    "        x = linear(x, state_dict[f'layer{li}.mlp_fc1'])\n",
    "        x = [xi.relu() for xi in x]\n",
    "        x = linear(x, state_dict[f'layer{li}.mlp_fc2'])\n",
    "        x = [a + b for a, b in zip(x, x_residual)]\n",
    "\n",
    "    logits = linear(x, state_dict['lm_head'])\n",
    "    return logits\n",
    "\n",
    "# Let there be Adam, the blessed optimizer and its buffers\n",
    "learning_rate, beta1, beta2, eps_adam = 0.01, 0.85, 0.99, 1e-8\n",
    "m = [0.0] * len(params) # first moment buffer\n",
    "v = [0.0] * len(params) # second moment buffer\n",
    "\n",
    "# Repeat in sequence\n",
    "num_steps = 1000 # number of training steps\n",
    "for step in range(num_steps):\n",
    "\n",
    "    # Take single document, tokenize it, surround it with BOS special token on both sides\n",
    "    doc = docs[step % len(docs)]\n",
    "    tokens = [BOS] + [uchars.index(ch) for ch in doc] + [BOS]\n",
    "    n = min(block_size, len(tokens) - 1)\n",
    "\n",
    "    # Forward the token sequence through the model, building up the computation graph all the way to the loss.\n",
    "    keys, values = [[] for _ in range(n_layer)], [[] for _ in range(n_layer)]\n",
    "    losses = []\n",
    "    for pos_id in range(n):\n",
    "        token_id, target_id = tokens[pos_id], tokens[pos_id + 1]\n",
    "        logits = gpt(token_id, pos_id, keys, values)\n",
    "        probs = softmax(logits)\n",
    "        loss_t = -probs[target_id].log()\n",
    "        losses.append(loss_t)\n",
    "    loss = (1 / n) * sum(losses) # final average loss over the document sequence. May yours be low.\n",
    "\n",
    "    # Backward the loss, calculating the gradients with respect to all model parameters.\n",
    "    loss.backward()\n",
    "\n",
    "    # Adam optimizer update: update the model parameters based on the corresponding gradients.\n",
    "    lr_t = learning_rate * (1 - step / num_steps) # linear learning rate decay\n",
    "    for i, p in enumerate(params):\n",
    "        m[i] = beta1 * m[i] + (1 - beta1) * p.grad\n",
    "        v[i] = beta2 * v[i] + (1 - beta2) * p.grad ** 2\n",
    "        m_hat = m[i] / (1 - beta1 ** (step + 1))\n",
    "        v_hat = v[i] / (1 - beta2 ** (step + 1))\n",
    "        p.data -= lr_t * m_hat / (v_hat ** 0.5 + eps_adam)\n",
    "        p.grad = 0\n",
    "\n",
    "    print(f\"step {step+1:4d} / {num_steps:4d} | loss {loss.data:.4f}\")\n",
    "\n",
    "# Inference: may the model babble back to us\n",
    "temperature = 0.5 # in (0, 1], control the \"creativity\" of generated text, low to high\n",
    "print(\"\\n--- inference (new, hallucinated names) ---\")\n",
    "for sample_idx in range(20):\n",
    "    keys, values = [[] for _ in range(n_layer)], [[] for _ in range(n_layer)]\n",
    "    token_id = BOS\n",
    "    sample = []\n",
    "    for pos_id in range(block_size):\n",
    "        logits = gpt(token_id, pos_id, keys, values)\n",
    "        probs = softmax([l / temperature for l in logits])\n",
    "        token_id = random.choices(range(vocab_size), weights=[p.data for p in probs])[0]\n",
    "        if token_id == BOS:\n",
    "            break\n",
    "        sample.append(uchars[token_id])\n",
    "    print(f\"sample {sample_idx+1:2d}: {''.join(sample)}\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "SageMath 10.6",
   "language": "sage",
   "name": "sagemath"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "sage",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
